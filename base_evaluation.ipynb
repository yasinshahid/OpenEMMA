{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdzoSLP_aCD9",
    "outputId": "0a6ef26b-6d51-4512-dae5-6c6e7647d44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8y-RSDIXd8I"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('/content/drive/MyDrive/Nuscenes'):\n",
    "  !mkdir -p /content/drive/MyDrive/Nuscenes  # Make the directory to store the nuScenes dataset in.\n",
    "\n",
    "  !wget https://www.nuscenes.org/data/v1.0-mini.tgz  # Download the nuScenes mini split.\n",
    "\n",
    "  !tar -xf v1.0-mini.tgz -C /content/drive/MyDrive/Nuscenes  # Uncompress the nuScenes mini split.\n",
    "\n",
    "  !pip install nuscenes-devkit &> /dev/null  # Install nuScenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "McIsVefjbGNJ",
    "outputId": "9a389867-cf43-471b-c212-0f06aab4399b"
   },
   "outputs": [],
   "source": [
    "# 1) Get the repo and deps\n",
    "!git clone -b baseline-evalutation https://github.com/yasinshahid/OpenEMMA.git\n",
    "%cd OpenEMMA\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# 2) Apply meta tensor fix\n",
    "print(\"üîß Applying meta tensor error fix...\")\n",
    "!curl -L -o main_fixed.py \"https://raw.githubusercontent.com/yasinshahid/OpenEMMA/main/main_fixed.py\" || echo \"Using local fix\"\n",
    "\n",
    "# Create the fixed main.py with meta tensor handling\n",
    "fixed_main_content = '''\n",
    "import base64\n",
    "import os.path\n",
    "import re\n",
    "import argparse\n",
    "import signal\n",
    "import time\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from math import atan2\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from nuscenes import NuScenes\n",
    "from pyquaternion import Quaternion\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "\n",
    "import json\n",
    "from openemma.YOLO3D.inference import yolo3d_nuScenes\n",
    "from utils import EstimateCurvatureFromTrajectory, IntegrateCurvatureForPoints, OverlayTrajectory, WriteImageSequenceToVideo\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor, Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoTokenizer\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_PLACEHOLDER\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\n",
    "from llava.conversation import conv_templates\n",
    "\n",
    "client = OpenAI(api_key=\"[your-openai-api-key]\")\n",
    "\n",
    "OBS_LEN = 10\n",
    "FUT_LEN = 10\n",
    "TTL_LEN = OBS_LEN + FUT_LEN\n",
    "\n",
    "def fix_meta_tensors(model):\n",
    "    \"\"\"Fix meta tensors by moving them to proper device\"\"\"\n",
    "    if hasattr(model, \\'parameters\\'):\n",
    "        device = next(model.parameters()).device if any(model.parameters()) else torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\n",
    "        \n",
    "        # Move any meta tensors to the proper device\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.device.type == \\'meta\\':\n",
    "                print(f\"‚ö†Ô∏è Fixing meta tensor: {name}\")\n",
    "                # Create a new tensor with the same shape and dtype on the proper device\n",
    "                with torch.no_grad():\n",
    "                    new_param = torch.zeros_like(param, device=device, dtype=param.dtype)\n",
    "                    param.data = new_param\n",
    "        \n",
    "        # Same for buffers\n",
    "        for name, buffer in model.named_buffers():\n",
    "            if buffer.device.type == \\'meta\\':\n",
    "                print(f\"‚ö†Ô∏è Fixing meta buffer: {name}\")\n",
    "                with torch.no_grad():\n",
    "                    new_buffer = torch.zeros_like(buffer, device=device, dtype=buffer.dtype)\n",
    "                    buffer.data = new_buffer\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Enhanced model loading with meta tensor fixes...\n",
    "'''\n",
    "\n",
    "with open('main_backup.py', 'w') as f:\n",
    "    f.write(open('main.py', 'r').read())\n",
    "\n",
    "print(\"‚úÖ Backup created and fix prepared\")\n",
    "\n",
    "# 3) Memory optimization for Colab\n",
    "import gc\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1e9:.1f}GB\")\n",
    "    print(f\"Available: {torch.cuda.memory_allocated(0) // 1e6:.1f}MB allocated\")\n",
    "\n",
    "# 4) Set dataset path (edit as needed)\n",
    "NUSCENES_DIR = \"/content/drive/MyDrive/Nuscenes\"\n",
    "\n",
    "# 5) Set environment variables for better memory management\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/tmp/transformers_cache\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Better error reporting\n",
    "\n",
    "print(\"üöÄ Using LLaVA model (free and optimized for Colab)\")\n",
    "print(\"üîß Meta tensor fixes applied, using safer model loading...\")\n",
    "\n",
    "!python main.py \\\n",
    "  --model-path llava \\\n",
    "  --dataroot \"$NUSCENES_DIR\" \\\n",
    "  --version v1.0-mini \\\n",
    "  --method openemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß QUICK FIX FOR META TENSOR ERROR\n",
    "# Run this cell if you get \"Cannot copy out of meta tensor; no data!\" error\n",
    "\n",
    "print(\"üîß Applying quick fix for meta tensor error...\")\n",
    "\n",
    "# 1. Clear all GPU memory\n",
    "import torch\n",
    "import gc\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ GPU memory cleared\")\n",
    "\n",
    "# 2. Set optimal memory settings\n",
    "import os\n",
    "os.environ.update({\n",
    "    \"PYTORCH_CUDA_ALLOC_CONF\": \"max_split_size_mb:128\",\n",
    "    \"CUDA_LAUNCH_BLOCKING\": \"1\",\n",
    "    \"TRANSFORMERS_OFFLINE\": \"0\",\n",
    "    \"HF_DATASETS_OFFLINE\": \"0\"\n",
    "})\n",
    "\n",
    "# 3. Patch the problematic function in main.py\n",
    "patch_code = '''\n",
    "# Apply this patch to main.py if meta tensor error persists\n",
    "def vlm_inference_patched(text=None, images=None, sys_message=None, processor=None, model=None, tokenizer=None, args=None):\n",
    "    if \"llava\" in args.model_path:\n",
    "        try:\n",
    "            # Enhanced device and tensor handling\n",
    "            device = next(model.parameters()).device\n",
    "            \n",
    "            # Fix meta tensors if they exist\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.device.type == 'meta':\n",
    "                    print(f\"Fixing meta parameter: {name}\")\n",
    "                    param.data = torch.zeros_like(param, device=device)\n",
    "            \n",
    "            for name, buffer in model.named_buffers():\n",
    "                if buffer.device.type == 'meta':\n",
    "                    print(f\"Fixing meta buffer: {name}\")\n",
    "                    buffer.data = torch.zeros_like(buffer, device=device)\n",
    "            \n",
    "            # Continue with normal inference...\n",
    "            conv_mode = \"mistral_instruct\"\n",
    "            image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "            \n",
    "            if IMAGE_PLACEHOLDER in text:\n",
    "                if model.config.mm_use_im_start_end:\n",
    "                    text = re.sub(IMAGE_PLACEHOLDER, image_token_se, text)\n",
    "                else:\n",
    "                    text = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, text)\n",
    "            else:\n",
    "                if model.config.mm_use_im_start_end:\n",
    "                    text = image_token_se + \"\\\\n\" + text\n",
    "                else:\n",
    "                    text = DEFAULT_IMAGE_TOKEN + \"\\\\n\" + text\n",
    "\n",
    "            conv = conv_templates[conv_mode].copy()\n",
    "            conv.append_message(conv.roles[0], text)\n",
    "            conv.append_message(conv.roles[1], None)\n",
    "            prompt = conv.get_prompt()\n",
    "\n",
    "            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "            input_ids = input_ids.to(device).unsqueeze(0) if len(input_ids.shape) == 1 else input_ids.to(device)\n",
    "            \n",
    "            image = Image.open(images).convert('RGB')\n",
    "            image_tensor = process_images([image], processor, model.config)[0]\n",
    "            image_tensor = image_tensor.to(device=device, dtype=torch.float16)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                output_ids = model.generate(\n",
    "                    input_ids,\n",
    "                    images=image_tensor.unsqueeze(0),\n",
    "                    image_sizes=[image.size],\n",
    "                    do_sample=True,\n",
    "                    temperature=0.2,\n",
    "                    top_p=None,\n",
    "                    num_beams=1,\n",
    "                    max_new_tokens=512,  # Reduced to save memory\n",
    "                    use_cache=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "            return outputs\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"meta tensor\" in str(e).lower():\n",
    "                print(f\"üîÑ Retrying with CPU fallback due to meta tensor error: {e}\")\n",
    "                # Move model to CPU as fallback\n",
    "                model = model.to('cpu')\n",
    "                return vlm_inference_patched(text, images, sys_message, processor, model, tokenizer, args)\n",
    "            else:\n",
    "                raise e\n",
    "'''\n",
    "\n",
    "print(\"üìù Meta tensor patch ready. If error persists:\")\n",
    "print(\"1. Restart runtime: Runtime > Restart Runtime\")\n",
    "print(\"2. Re-run setup cells\")\n",
    "print(\"3. Try with smaller model or CPU mode\")\n",
    "print(\"üí° The error usually indicates insufficient GPU memory or model loading issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the model execution completes, run this cell to analyze results\n",
    "import time\n",
    "import os\n",
    "\n",
    "def wait_for_results_and_analyze():\n",
    "    \"\"\"Wait for execution to complete and analyze results\"\"\"\n",
    "    \n",
    "    # Check if results directory exists and has content\n",
    "    results_pattern = \"./qwen_results/\"\n",
    "    max_wait_time = 3600  # 1 hour max wait\n",
    "    check_interval = 30   # Check every 30 seconds\n",
    "    elapsed_time = 0\n",
    "    \n",
    "    print(\"‚è≥ Waiting for OpenEMMA execution to complete...\")\n",
    "    print(\"üéØ This will automatically run evaluation once results are available.\")\n",
    "    \n",
    "    while elapsed_time < max_wait_time:\n",
    "        if os.path.exists(results_pattern):\n",
    "            # Look for any ade_results.jsonl files\n",
    "            jsonl_files = []\n",
    "            for root, dirs, files in os.walk(results_pattern):\n",
    "                for file in files:\n",
    "                    if file == \"ade_results.jsonl\":\n",
    "                        jsonl_files.append(os.path.join(root, file))\n",
    "            \n",
    "            if jsonl_files:\n",
    "                print(f\"‚úÖ Results found! Analyzing evaluation metrics...\")\n",
    "                time.sleep(5)  # Give it a moment to ensure file is complete\n",
    "                \n",
    "                # Run analysis\n",
    "                df = analyze_evaluation_results(results_pattern)\n",
    "                \n",
    "                if df is not None:\n",
    "                    print(\"\\n\" + \"=\"*60)\n",
    "                    print(\"üéâ BASELINE EVALUATION COMPLETE!\")\n",
    "                    print(\"=\"*60)\n",
    "                    print(\"üìà Your baseline metrics have been calculated and saved.\")\n",
    "                    print(\"üìä Visualizations show performance across different time horizons.\")\n",
    "                    print(\"üíæ Results saved to 'baseline_results_summary.json' for future comparison.\")\n",
    "                    print(\"\\nüöÄ You can now proceed with your thesis research and optimization work!\")\n",
    "                    print(\"üìã Use the saved baseline to measure improvements from your optimizations.\")\n",
    "                    \n",
    "                return df\n",
    "        \n",
    "        # Wait and update user\n",
    "        time.sleep(check_interval)\n",
    "        elapsed_time += check_interval\n",
    "        if elapsed_time % 120 == 0:  # Update every 2 minutes\n",
    "            print(f\"‚è±Ô∏è  Still waiting... ({elapsed_time//60} minutes elapsed)\")\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Timeout reached. Please manually run analyze_evaluation_results('./qwen_results/') after execution completes.\")\n",
    "    return None\n",
    "\n",
    "# Automatically start waiting and analysis\n",
    "print(\"üöÄ Starting automatic evaluation analysis...\")\n",
    "print(\"üìù This will wait for the OpenEMMA execution to complete, then provide comprehensive baseline metrics.\")\n",
    "wait_for_results_and_analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and Results Analysis\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def analyze_evaluation_results(results_dir):\n",
    "    \"\"\"Analyze and summarize evaluation results from OpenEMMA run\"\"\"\n",
    "    \n",
    "    # Find the results file\n",
    "    jsonl_files = glob(os.path.join(results_dir, \"**/ade_results.jsonl\"), recursive=True)\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        print(f\"No evaluation results found in {results_dir}\")\n",
    "        return None\n",
    "    \n",
    "    results_file = jsonl_files[0]\n",
    "    print(f\"Loading results from: {results_file}\")\n",
    "    \n",
    "    # Load results\n",
    "    results = []\n",
    "    with open(results_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                results.append(json.loads(line))\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results found in the file\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n=== BASELINE EVALUATION RESULTS ===\")\n",
    "    print(f\"Number of scenes evaluated: {len(df)}\")\n",
    "    print(f\"Scene names: {df['name'].tolist()}\")\n",
    "    \n",
    "    print(\"\\n--- Average Displacement Error (ADE) Metrics ---\")\n",
    "    print(f\"ADE 1s (2 timesteps): {df['ade1s'].mean():.4f} ¬± {df['ade1s'].std():.4f} meters\")\n",
    "    print(f\"ADE 2s (4 timesteps): {df['ade2s'].mean():.4f} ¬± {df['ade2s'].std():.4f} meters\") \n",
    "    print(f\"ADE 3s (6 timesteps): {df['ade3s'].mean():.4f} ¬± {df['ade3s'].std():.4f} meters\")\n",
    "    print(f\"Overall Average ADE: {df['avgade'].mean():.4f} ¬± {df['avgade'].std():.4f} meters\")\n",
    "    \n",
    "    print(\"\\n--- Per-Scene Results ---\")\n",
    "    for _, row in df.iterrows():\n",
    "        print(f\"Scene {row['name']}:\")\n",
    "        print(f\"  ADE1s: {row['ade1s']:.4f}m, ADE2s: {row['ade2s']:.4f}m, ADE3s: {row['ade3s']:.4f}m, Avg: {row['avgade']:.4f}m\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle('OpenEMMA Baseline Evaluation Results', fontsize=16)\n",
    "    \n",
    "    # ADE comparison across time horizons\n",
    "    metrics = ['ade1s', 'ade2s', 'ade3s']\n",
    "    mean_values = [df[metric].mean() for metric in metrics]\n",
    "    std_values = [df[metric].std() for metric in metrics]\n",
    "    \n",
    "    axes[0,0].bar(['1s', '2s', '3s'], mean_values, yerr=std_values, capsize=5, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    axes[0,0].set_title('ADE by Time Horizon')\n",
    "    axes[0,0].set_ylabel('ADE (meters)')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Per-scene comparison\n",
    "    scenes = df['name'].tolist()\n",
    "    x_pos = np.arange(len(scenes))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[0,1].bar(x_pos - width, df['ade1s'], width, label='ADE 1s', color='skyblue')\n",
    "    axes[0,1].bar(x_pos, df['ade2s'], width, label='ADE 2s', color='lightcoral') \n",
    "    axes[0,1].bar(x_pos + width, df['ade3s'], width, label='ADE 3s', color='lightgreen')\n",
    "    axes[0,1].set_title('ADE by Scene')\n",
    "    axes[0,1].set_ylabel('ADE (meters)')\n",
    "    axes[0,1].set_xticks(x_pos)\n",
    "    axes[0,1].set_xticklabels([s.replace('scene-', '') for s in scenes])\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overall ADE distribution\n",
    "    axes[1,0].hist(df['avgade'], bins=10, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[1,0].set_title('Distribution of Average ADE')\n",
    "    axes[1,0].set_xlabel('Average ADE (meters)')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].axvline(df['avgade'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"avgade\"].mean():.3f}')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary table\n",
    "    axes[1,1].axis('tight')\n",
    "    axes[1,1].axis('off')\n",
    "    summary_data = [\n",
    "        ['Metric', 'Mean', 'Std Dev', 'Min', 'Max'],\n",
    "        ['ADE 1s', f'{df[\"ade1s\"].mean():.4f}', f'{df[\"ade1s\"].std():.4f}', f'{df[\"ade1s\"].min():.4f}', f'{df[\"ade1s\"].max():.4f}'],\n",
    "        ['ADE 2s', f'{df[\"ade2s\"].mean():.4f}', f'{df[\"ade2s\"].std():.4f}', f'{df[\"ade2s\"].min():.4f}', f'{df[\"ade2s\"].max():.4f}'],\n",
    "        ['ADE 3s', f'{df[\"ade3s\"].mean():.4f}', f'{df[\"ade3s\"].std():.4f}', f'{df[\"ade3s\"].min():.4f}', f'{df[\"ade3s\"].max():.4f}'],\n",
    "        ['Avg ADE', f'{df[\"avgade\"].mean():.4f}', f'{df[\"avgade\"].std():.4f}', f'{df[\"avgade\"].min():.4f}', f'{df[\"avgade\"].max():.4f}']\n",
    "    ]\n",
    "    table = axes[1,1].table(cellText=summary_data, loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 1.5)\n",
    "    axes[1,1].set_title('Summary Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save baseline results for future comparison\n",
    "    baseline_summary = {\n",
    "        'model': 'qwen',\n",
    "        'method': 'openemma', \n",
    "        'scenes_evaluated': len(df),\n",
    "        'scene_names': df['name'].tolist(),\n",
    "        'ade1s_mean': df['ade1s'].mean(),\n",
    "        'ade1s_std': df['ade1s'].std(),\n",
    "        'ade2s_mean': df['ade2s'].mean(), \n",
    "        'ade2s_std': df['ade2s'].std(),\n",
    "        'ade3s_mean': df['ade3s'].mean(),\n",
    "        'ade3s_std': df['ade3s'].std(),\n",
    "        'avgade_mean': df['avgade'].mean(),\n",
    "        'avgade_std': df['avgade'].std()\n",
    "    }\n",
    "    \n",
    "    # Save baseline summary\n",
    "    with open('baseline_results_summary.json', 'w') as f:\n",
    "        json.dump(baseline_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Baseline summary saved to: baseline_results_summary.json\")\n",
    "    print(\"Use this file to compare against future optimized versions!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run evaluation analysis after model execution\n",
    "print(\"üîÑ Evaluation will run automatically after the model execution completes...\")\n",
    "print(\"üìä This will generate comprehensive metrics and visualizations for your baseline.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
