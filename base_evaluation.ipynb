{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdzoSLP_aCD9",
    "outputId": "0a6ef26b-6d51-4512-dae5-6c6e7647d44c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8y-RSDIXd8I"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('/content/drive/MyDrive/Nuscenes'):\n",
    "  !mkdir -p /content/drive/MyDrive/Nuscenes  # Make the directory to store the nuScenes dataset in.\n",
    "\n",
    "  !wget https://www.nuscenes.org/data/v1.0-mini.tgz  # Download the nuScenes mini split.\n",
    "\n",
    "  !tar -xf v1.0-mini.tgz -C /content/drive/MyDrive/Nuscenes  # Uncompress the nuScenes mini split.\n",
    "\n",
    "  !pip install nuscenes-devkit &> /dev/null  # Install nuScenes."
   ]
  },
  {
   "cell_type": "code",
   "source": "# 1) Get the repo and deps\n!git clone -b baseline-evalutation https://github.com/yasinshahid/OpenEMMA.git\n%cd OpenEMMA\n!python -m pip install --upgrade pip\n!pip install --no-cache-dir -r requirements.txt\n\n# 3) Set dataset path (edit as needed)\nNUSCENES_DIR = \"/content/drive/MyDrive/Nuscenes\"\n\n# 4) Run the main script with LLaVA (FREE and lighter model)\nprint(\"üöÄ Using LLaVA model (free and optimized for Colab)\")\n!python main.py \\\n  --model-path llava \\\n  --dataroot \"$NUSCENES_DIR\" \\\n  --version v1.0-mini \\\n  --method openemma",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "McIsVefjbGNJ",
    "outputId": "9a389867-cf43-471b-c212-0f06aab4399b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# After the model execution completes, run this cell to analyze results\nimport time\nimport os\n\ndef wait_for_results_and_analyze():\n    \"\"\"Wait for execution to complete and analyze results\"\"\"\n    \n    # Check if results directory exists and has content\n    results_pattern = \"./qwen_results/\"\n    max_wait_time = 3600  # 1 hour max wait\n    check_interval = 30   # Check every 30 seconds\n    elapsed_time = 0\n    \n    print(\"‚è≥ Waiting for OpenEMMA execution to complete...\")\n    print(\"üéØ This will automatically run evaluation once results are available.\")\n    \n    while elapsed_time < max_wait_time:\n        if os.path.exists(results_pattern):\n            # Look for any ade_results.jsonl files\n            jsonl_files = []\n            for root, dirs, files in os.walk(results_pattern):\n                for file in files:\n                    if file == \"ade_results.jsonl\":\n                        jsonl_files.append(os.path.join(root, file))\n            \n            if jsonl_files:\n                print(f\"‚úÖ Results found! Analyzing evaluation metrics...\")\n                time.sleep(5)  # Give it a moment to ensure file is complete\n                \n                # Run analysis\n                df = analyze_evaluation_results(results_pattern)\n                \n                if df is not None:\n                    print(\"\\n\" + \"=\"*60)\n                    print(\"üéâ BASELINE EVALUATION COMPLETE!\")\n                    print(\"=\"*60)\n                    print(\"üìà Your baseline metrics have been calculated and saved.\")\n                    print(\"üìä Visualizations show performance across different time horizons.\")\n                    print(\"üíæ Results saved to 'baseline_results_summary.json' for future comparison.\")\n                    print(\"\\nüöÄ You can now proceed with your thesis research and optimization work!\")\n                    print(\"üìã Use the saved baseline to measure improvements from your optimizations.\")\n                    \n                return df\n        \n        # Wait and update user\n        time.sleep(check_interval)\n        elapsed_time += check_interval\n        if elapsed_time % 120 == 0:  # Update every 2 minutes\n            print(f\"‚è±Ô∏è  Still waiting... ({elapsed_time//60} minutes elapsed)\")\n    \n    print(\"‚ö†Ô∏è Timeout reached. Please manually run analyze_evaluation_results('./qwen_results/') after execution completes.\")\n    return None\n\n# Automatically start waiting and analysis\nprint(\"üöÄ Starting automatic evaluation analysis...\")\nprint(\"üìù This will wait for the OpenEMMA execution to complete, then provide comprehensive baseline metrics.\")\nwait_for_results_and_analyze()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Evaluation and Results Analysis\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport os\n\ndef analyze_evaluation_results(results_dir):\n    \"\"\"Analyze and summarize evaluation results from OpenEMMA run\"\"\"\n    \n    # Find the results file\n    jsonl_files = glob(os.path.join(results_dir, \"**/ade_results.jsonl\"), recursive=True)\n    \n    if not jsonl_files:\n        print(f\"No evaluation results found in {results_dir}\")\n        return None\n    \n    results_file = jsonl_files[0]\n    print(f\"Loading results from: {results_file}\")\n    \n    # Load results\n    results = []\n    with open(results_file, 'r') as f:\n        for line in f:\n            if line.strip():\n                results.append(json.loads(line))\n    \n    if not results:\n        print(\"No results found in the file\")\n        return None\n    \n    # Convert to DataFrame for analysis\n    df = pd.DataFrame(results)\n    \n    # Summary statistics\n    print(\"\\n=== BASELINE EVALUATION RESULTS ===\")\n    print(f\"Number of scenes evaluated: {len(df)}\")\n    print(f\"Scene names: {df['name'].tolist()}\")\n    \n    print(\"\\n--- Average Displacement Error (ADE) Metrics ---\")\n    print(f\"ADE 1s (2 timesteps): {df['ade1s'].mean():.4f} ¬± {df['ade1s'].std():.4f} meters\")\n    print(f\"ADE 2s (4 timesteps): {df['ade2s'].mean():.4f} ¬± {df['ade2s'].std():.4f} meters\") \n    print(f\"ADE 3s (6 timesteps): {df['ade3s'].mean():.4f} ¬± {df['ade3s'].std():.4f} meters\")\n    print(f\"Overall Average ADE: {df['avgade'].mean():.4f} ¬± {df['avgade'].std():.4f} meters\")\n    \n    print(\"\\n--- Per-Scene Results ---\")\n    for _, row in df.iterrows():\n        print(f\"Scene {row['name']}:\")\n        print(f\"  ADE1s: {row['ade1s']:.4f}m, ADE2s: {row['ade2s']:.4f}m, ADE3s: {row['ade3s']:.4f}m, Avg: {row['avgade']:.4f}m\")\n    \n    # Create visualization\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n    fig.suptitle('OpenEMMA Baseline Evaluation Results', fontsize=16)\n    \n    # ADE comparison across time horizons\n    metrics = ['ade1s', 'ade2s', 'ade3s']\n    mean_values = [df[metric].mean() for metric in metrics]\n    std_values = [df[metric].std() for metric in metrics]\n    \n    axes[0,0].bar(['1s', '2s', '3s'], mean_values, yerr=std_values, capsize=5, color=['skyblue', 'lightcoral', 'lightgreen'])\n    axes[0,0].set_title('ADE by Time Horizon')\n    axes[0,0].set_ylabel('ADE (meters)')\n    axes[0,0].grid(True, alpha=0.3)\n    \n    # Per-scene comparison\n    scenes = df['name'].tolist()\n    x_pos = np.arange(len(scenes))\n    width = 0.25\n    \n    axes[0,1].bar(x_pos - width, df['ade1s'], width, label='ADE 1s', color='skyblue')\n    axes[0,1].bar(x_pos, df['ade2s'], width, label='ADE 2s', color='lightcoral') \n    axes[0,1].bar(x_pos + width, df['ade3s'], width, label='ADE 3s', color='lightgreen')\n    axes[0,1].set_title('ADE by Scene')\n    axes[0,1].set_ylabel('ADE (meters)')\n    axes[0,1].set_xticks(x_pos)\n    axes[0,1].set_xticklabels([s.replace('scene-', '') for s in scenes])\n    axes[0,1].legend()\n    axes[0,1].grid(True, alpha=0.3)\n    \n    # Overall ADE distribution\n    axes[1,0].hist(df['avgade'], bins=10, alpha=0.7, color='purple', edgecolor='black')\n    axes[1,0].set_title('Distribution of Average ADE')\n    axes[1,0].set_xlabel('Average ADE (meters)')\n    axes[1,0].set_ylabel('Frequency')\n    axes[1,0].axvline(df['avgade'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"avgade\"].mean():.3f}')\n    axes[1,0].legend()\n    axes[1,0].grid(True, alpha=0.3)\n    \n    # Summary table\n    axes[1,1].axis('tight')\n    axes[1,1].axis('off')\n    summary_data = [\n        ['Metric', 'Mean', 'Std Dev', 'Min', 'Max'],\n        ['ADE 1s', f'{df[\"ade1s\"].mean():.4f}', f'{df[\"ade1s\"].std():.4f}', f'{df[\"ade1s\"].min():.4f}', f'{df[\"ade1s\"].max():.4f}'],\n        ['ADE 2s', f'{df[\"ade2s\"].mean():.4f}', f'{df[\"ade2s\"].std():.4f}', f'{df[\"ade2s\"].min():.4f}', f'{df[\"ade2s\"].max():.4f}'],\n        ['ADE 3s', f'{df[\"ade3s\"].mean():.4f}', f'{df[\"ade3s\"].std():.4f}', f'{df[\"ade3s\"].min():.4f}', f'{df[\"ade3s\"].max():.4f}'],\n        ['Avg ADE', f'{df[\"avgade\"].mean():.4f}', f'{df[\"avgade\"].std():.4f}', f'{df[\"avgade\"].min():.4f}', f'{df[\"avgade\"].max():.4f}']\n    ]\n    table = axes[1,1].table(cellText=summary_data, loc='center', cellLoc='center')\n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1, 1.5)\n    axes[1,1].set_title('Summary Statistics')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Save baseline results for future comparison\n    baseline_summary = {\n        'model': 'qwen',\n        'method': 'openemma', \n        'scenes_evaluated': len(df),\n        'scene_names': df['name'].tolist(),\n        'ade1s_mean': df['ade1s'].mean(),\n        'ade1s_std': df['ade1s'].std(),\n        'ade2s_mean': df['ade2s'].mean(), \n        'ade2s_std': df['ade2s'].std(),\n        'ade3s_mean': df['ade3s'].mean(),\n        'ade3s_std': df['ade3s'].std(),\n        'avgade_mean': df['avgade'].mean(),\n        'avgade_std': df['avgade'].std()\n    }\n    \n    # Save baseline summary\n    with open('baseline_results_summary.json', 'w') as f:\n        json.dump(baseline_summary, f, indent=2)\n    \n    print(f\"\\n‚úÖ Baseline summary saved to: baseline_results_summary.json\")\n    print(\"Use this file to compare against future optimized versions!\")\n    \n    return df\n\n# Run evaluation analysis after model execution\nprint(\"üîÑ Evaluation will run automatically after the model execution completes...\")\nprint(\"üìä This will generate comprehensive metrics and visualizations for your baseline.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}